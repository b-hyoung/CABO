  2. '커밋왕' 같은 랭킹 기능 구현 아이디어

  '커밋왕' 기능은 사용자들의 재미와 경쟁심을 유발하는 아주 좋은 기능입니다. 직접 구현하려면 백엔드(서버)와
  데이터베이스가 필요하며, 전체적인 흐름은 다음과 같습니다.

  `GitHub API` → `백엔드 서버` → `데이터베이스` → `API` → `프론트엔드`

  구현 단계별 아이디어

  1단계: 데이터 수집 (GitHub API 연동)

   * 먼저 특정 사용자의 커밋 기록을 가져와야 합니다. GitHub API의 [Events
     API](https://docs.github.com/en/rest/activity/events)를 사용하면 사용자의 활동 기록(PushEvent, 즉 커밋)을 가져올 수
     있습니다.
   * 백엔드 서버(예: Python FastAPI)에서 이 API를 호출하여 지난 일주일 또는 한 달간의 커밋 데이터를 수집합니다.
   * 주의할 점: GitHub API는 시간당 요청 횟수가 정해져 있으므로, 너무 자주 호출하지 않도록 주의해야 합니다.

  2단계: 데이터 저장 (데이터베이스 설계)

   * 매번 랭킹을 보여줄 때마다 GitHub API를 호출하는 것은 매우 느리고 비효율적입니다. 따라서 수집한 데이터를 자체
     데이터베이스(DB)에 저장해야 합니다.
   * Users 테이블 외에, DailyCommits 같은 테이블을 만들어 사용자별로 매일의 커밋 횟수를 저장해두는 것이 좋습니다.
       * 예: (user_id, date, commit_count)

  3단계: 랭킹 집계 (스케줄링 작업)

   * '커밋왕' 랭킹을 실시간으로 계산하는 것은 서버에 부담을 줍니다. 대신 하루에 한 번 등 정해진 시간에 미리 순위를
     계산해두는 것이 좋습니다.
   * 이것을 스케줄링 작업(Scheduled Job 또는 Cron Job) 이라고 부릅니다.
   * 이 작업은 매일 자정에 실행되면서, 데이터베이스에 저장된 커밋 기록을 바탕으로 '최근 7일간 가장 커밋을 많이 한 사용자
     TOP 100'을 계산하여 WeeklyRanking 같은 별도의 테이블에 저장합니다.

  4단계: 랭킹 보여주기 (API 제공)

   * 이제 프론트엔드(현재의 Next.js 페이지)에서는 랭킹을 보여달라고 요청만 하면 됩니다.
   * 백엔드에서는 미리 계산해 둔 WeeklyRanking 테이블에서 순위를 가져와 즉시 전달해줍니다.
   * 이렇게 하면 사용자는 기다림 없이 매우 빠르게 '커밋왕' 랭킹 보드를 볼 수 있습니다.

  요약

  '커밋왕' 기능의 핵심은 "데이터를 미리 수집하고, 정해진 시간에 미리 계산해서, 요청 시에는 계산된 결과만 빠르게 보여주는
  것" 입니다. 이 구조를 이해하시면 '코드 품질왕', '리뷰왕' 등 다양한 랭킹 시스템을 확장해서 만드실 수 있습니다.

> 내꺼 git을 기준으로 실험할껀데 전체 깃허브 데이터 중 어느정도 데이터를 가져올 수 있으며 어떤 데이터를 가져와야 한다고 
  생각 해 ?

✦ 네, 본인 깃허브를 기준으로 실험하는 것은 아주 좋은 방법입니다. 어떤 데이터를, 얼마나 가져와야 할지 프로젝트의 핵심    
  기능에 맞춰 설명해 드리겠습니다.

  데이터의 양: 얼마나 가져올 수 있는가?

  가장 중요한 것은 GitHub API의 '시간당 요청 횟수 제한(Rate Limiting)'입니다.                                           
   - 인증 없이 요청: 시간당 60회                                                                                        
   - 인증 토큰 사용 시: 시간당 5,000회                                                                                  
                                                                                                                        
  따라서 개발하실 때는 반드시 GitHub의 개인용 액세스 토큰(Personal Access Token)을 발급받아 사용하셔야 원활한 테스트가
  가능합니다. 인증을 거치면 개인 프로젝트 분석에 필요한 데이터를 충분히 가져올 수 있습니다.

  어떤 데이터를 가져와야 하는가? (기능별 핵심 데이터)

  ReadMe.md에 정의된 기능들을 구현하기 위해 꼭 필요한 데이터와 API 엔드포인트를 우선순위 순으로 정리했습니다.

  [1순위 - 필수] 기본 프로필과 사용 언어 분석

  이 데이터만 있어도 '프로필 카드'와 '주요 사용 언어' 기능을 구현할 수 있습니다.

   1. 사용자 기본 정보 가져오기
       * 목적: 프로필 이미지, 이름 등 기본 정보 표시
       * API: GET /users/{username}
       * 핵심 데이터: avatar_url, name, login, public_repos

   2. 리포지토리 목록 가져오기
       * 목적: 최근 활동한 리포지토리를 찾고, 언어 사용량을 분석하기 위함
       * API: GET /users/{username}/repos
       * 전략: 리포지토리 목록을 받아온 뒤, `pushed_at` 값을 기준으로 최근에 업데이트한 리포지토리 3~5개를 추려냅니다.  

   3. 리포지토리별 언어 데이터 가져오기
       * 목적: '주요 사용 언어'의 퍼센티지 계산
       * API: GET /repos/{username}/{repo}/languages
       * 핵심 데이터: { "TypeScript": 45000, "JavaScript": 30000, ... } 와 같이 언어별 코드의 바이트(byte) 수를
         반환합니다. 이걸 합산하여 전체 언어 사용량 비율을 계산할 수 있습니다.

  [2순위 - 핵심] 활동 패턴 분석 (잠수함 탐지)

   1. 커밋 히스토리 가져오기
       * 목적: 커밋 주기와 시간대를 분석하여 성실성 파악
       * API: GET /repos/{username}/{repo}/commits
       * 핵심 데이터: commit.author.date (커밋 시간)
       * 전략: 위에서 추린 최근 리포지토리들의 커밋 기록을 가져와서, date 값을 분석합니다. (예: 주중/주말 커밋 비율,
         특정 시간대 집중도 등)

  [3순위 - 고급] 코드 및 협업 분석

  AI 분석이 필요한 부분이므로 시간과 비용이 가장 많이 드는 단계입니다.

   1. 소스코드 내용 가져오기
       * 목적: AI를 통한 코드 품질 분석 (변수명, 주석, 함수 구조 등)
       * API: GET /repos/{username}/{repo}/contents/{path}
       * 전략: 전체 코드를 분석하는 것은 비효율적입니다. 가장 최근 프로젝트의 핵심 소스 파일 1~2개의 내용만 가져와서
         AI에게 전달하여 분석을 테스트하는 것이 좋습니다.

   2. PR, Issue 기록 가져오기
       * 목적: 협업 스타일, 소통 방식 분석
       * API: GET /repos/{username}/{repo}/pulls, .../issues 등
       * 전략: 가장 활발했던 리포지토리의 최근 PR 5~10개에 달린 코멘트들을 분석용으로 가져옵니다. 이 기능은 여러 API를  
         호출해야 하므로 MVP(최소 기능 제품) 이후에 구현하는 것을 추천합니다.